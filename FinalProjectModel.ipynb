{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112fc221-038c-4d02-8501-fe54b17f0c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOAD DATA ===\n",
      "Rows / Cols: (4416, 72)\n",
      "\n",
      "=== FILTER: keep (% Villain Checks == -1) OR >= 85.0 ===\n",
      "Before: 4416 | After: 4267 | Dropped: 149\n",
      "Targets look like percents. Dividing by 100.\n",
      "\n",
      "Targets sanity check (first 5):\n",
      "      CHECK     BET13     BET23  sum\n",
      "0  0.079208  0.792079  0.128713  1.0\n",
      "1  0.030000  0.900000  0.070000  1.0\n",
      "2  0.030000  0.920000  0.050000  1.0\n",
      "3  0.030000  0.790000  0.180000  1.0\n",
      "4  0.040000  0.780000  0.180000  1.0\n",
      "\n",
      "=== ADD SEMANTIC FEATURES ===\n",
      "\n",
      "=== FINAL FEATURE MATRIX ===\n",
      "X shape: (4267, 32)\n",
      "First 25 features: ['HERO_NUTS', 'VIL_NUTS', 'NUTS_ADV', 'HERO_STRONG', 'VIL_STRONG', 'STRONG_ADV', 'HERO_TP_PLUS', 'VIL_TP_PLUS', 'TP_PLUS_ADV', 'HERO_AIR', 'VIL_AIR', 'AIR_ADV', 'DRAW_ADV', 'COMBO_DRAW_ADV', 'HERO_MEDIUM', 'VIL_MEDIUM', 'HERO_POLAR', 'VIL_POLAR', 'POLAR_ADV', 'SPR', 'Position_IP', 'Position_OOP', 'Full Houses_PAIRED', 'Full Houses_TRIPS', 'Full Houses_UNPAIRED']\n",
      "\n",
      "=== SPLITS (train/val/test) ===\n",
      "Train: (2559, 32) Val: (854, 32) Test: (854, 32)\n",
      "\n",
      "=== PRECOMPUTE GLOBAL THRESHOLDS (train only) ===\n",
      "Done. seconds=0.00 | per-feature thresholds=16\n",
      "\n",
      "=== TUNING SUBSET ===\n",
      "Using 1200 / 2559 training rows for tuning.\n",
      "\n",
      "=== TUNING TEACHER (pick by VAL loss; report seed variance) ===\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "CONFIG 1/8 A: {'n_estimators': 140, 'max_depth': 5, 'min_leaf': 60, 'max_features': 0.7, 'bootstrap': True, 'name': 'A'}\n",
      "  seed=42 | fit=9.75s | tune=0.022663 | val=0.023295 | oob=0.024475\n",
      "  seed=52 | fit=9.76s | tune=0.022699 | val=0.023309 | oob=0.024423\n",
      "  seed=62 | fit=9.89s | tune=0.022706 | val=0.023305 | oob=0.024474\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "CONFIG 2/8 B: {'n_estimators': 180, 'max_depth': 5, 'min_leaf': 60, 'max_features': 0.7, 'bootstrap': True, 'name': 'B'}\n",
      "  seed=42 | fit=12.60s | tune=0.022631 | val=0.023282 | oob=0.024384\n",
      "  seed=52 | fit=12.76s | tune=0.022756 | val=0.023369 | oob=0.024421\n",
      "  seed=62 | fit=13.07s | tune=0.022731 | val=0.023348 | oob=0.024459\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "CONFIG 3/8 C: {'n_estimators': 200, 'max_depth': 5, 'min_leaf': 60, 'max_features': 0.7, 'bootstrap': True, 'name': 'C'}\n",
      "  seed=42 | fit=14.07s | tune=0.022667 | val=0.023340 | oob=0.024433\n",
      "  seed=52 | fit=13.93s | tune=0.022773 | val=0.023406 | oob=0.024426\n",
      "  seed=62 | fit=14.13s | tune=0.022718 | val=0.023327 | oob=0.024439\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "CONFIG 4/8 D: {'n_estimators': 180, 'max_depth': 4, 'min_leaf': 60, 'max_features': 0.7, 'bootstrap': True, 'name': 'D'}\n",
      "  seed=42 | fit=11.55s | tune=0.024407 | val=0.024694 | oob=0.025891\n",
      "  seed=52 | fit=11.46s | tune=0.024365 | val=0.024628 | oob=0.025823\n",
      "  seed=62 | fit=11.55s | tune=0.024503 | val=0.024755 | oob=0.025995\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "CONFIG 5/8 E: {'n_estimators': 180, 'max_depth': 5, 'min_leaf': 80, 'max_features': 0.7, 'bootstrap': True, 'name': 'E'}\n",
      "  seed=42 | fit=9.97s | tune=0.024450 | val=0.025034 | oob=0.026021\n",
      "  seed=52 | fit=9.98s | tune=0.024478 | val=0.025008 | oob=0.026024\n",
      "  seed=62 | fit=10.06s | tune=0.024720 | val=0.025184 | oob=0.026202\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "CONFIG 6/8 F: {'n_estimators': 220, 'max_depth': 5, 'min_leaf': 60, 'max_features': 0.8, 'bootstrap': True, 'name': 'F'}\n",
      "  seed=42 | fit=18.09s | tune=0.022673 | val=0.023306 | oob=0.024462\n",
      "  seed=52 | fit=18.05s | tune=0.022675 | val=0.023238 | oob=0.024324\n",
      "  seed=62 | fit=18.71s | tune=0.022734 | val=0.023274 | oob=0.024445\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "CONFIG 7/8 G: {'n_estimators': 160, 'max_depth': 5, 'min_leaf': 60, 'max_features': 0.7, 'bootstrap': True, 'name': 'G'}\n",
      "  seed=42 | fit=11.22s | tune=0.022632 | val=0.023286 | oob=0.024426\n",
      "  seed=52 | fit=11.16s | tune=0.022732 | val=0.023356 | oob=0.024424\n",
      "  seed=62 | fit=11.26s | tune=0.022687 | val=0.023286 | oob=0.024412\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "CONFIG 8/8 H: {'n_estimators': 160, 'max_depth': 4, 'min_leaf': 60, 'max_features': 0.7, 'bootstrap': True, 'name': 'H'}\n",
      "  seed=42 | fit=10.11s | tune=0.024403 | val=0.024699 | oob=0.025947\n",
      "  seed=52 | fit=10.17s | tune=0.024325 | val=0.024598 | oob=0.025817\n",
      "  seed=62 | fit=10.20s | tune=0.024448 | val=0.024702 | oob=0.025938\n",
      "\n",
      "=== TEACHER TUNING SUMMARY (sorted by mean val loss, then stability) ===\n",
      "name  n_estimators  max_depth  min_leaf  max_features  bootstrap  val_loss_mean  val_loss_std  oob_loss_mean  fit_seconds_mean\n",
      "   F           220          5        60           0.8       True       0.023273      0.000028       0.024410         18.284132\n",
      "   A           140          5        60           0.7       True       0.023303      0.000006       0.024457          9.801785\n",
      "   G           160          5        60           0.7       True       0.023309      0.000033       0.024421         11.214662\n",
      "   B           180          5        60           0.7       True       0.023333      0.000037       0.024421         12.811798\n",
      "   C           200          5        60           0.7       True       0.023358      0.000035       0.024433         14.046415\n",
      "   H           160          4        60           0.7       True       0.024666      0.000048       0.025901         10.158892\n",
      "   D           180          4        60           0.7       True       0.024692      0.000052       0.025903         11.520622\n",
      "   E           180          5        80           0.7       True       0.025075      0.000078       0.026082         10.002671\n",
      "\n",
      "=== BEST TEACHER CONFIG (by VAL mean + stability) ===\n",
      "{'name': 'F', 'n_estimators': 220, 'max_depth': 5, 'min_leaf': 60, 'max_features': 0.8, 'bootstrap': True, 'val_loss_mean': 0.02327262582557801, 'val_loss_std': 2.7979927985067835e-05, 'oob_loss_mean': 0.024410234432100553, 'fit_seconds_mean': 18.284131685892742}\n",
      "\n",
      "======================================================================\n",
      "=== FINAL TEACHER (fit TRAIN+VAL) ===\n",
      "Teacher params: {'n_estimators': 240, 'max_depth': 5, 'min_leaf': 60, 'max_features': 0.8, 'bootstrap': True, 'thresholds_by_feature': [array([0.        , 0.0069532 , 0.01493373, 0.02777728]), array([0.        , 0.00829846, 0.0195141 , 0.03222852]), array([-0.0121223 , -0.00289841,  0.        ,  0.00430265]), array([0.01571896, 0.02425371, 0.02839028, 0.03429643, 0.03990831,\n",
      "       0.04610402, 0.05175649, 0.05906989, 0.06937098, 0.08177381,\n",
      "       0.10253556, 0.12670664, 0.17850667, 0.30347762, 0.39667018,\n",
      "       0.53723819]), array([0.02660626, 0.03391579, 0.03983383, 0.04476336, 0.04890573,\n",
      "       0.05417579, 0.0595372 , 0.06576791, 0.07273929, 0.08135542,\n",
      "       0.09689119, 0.12023923, 0.15238281, 0.28843214, 0.44978482,\n",
      "       0.55268416]), array([-0.10159023, -0.05326419, -0.03960963, -0.03115532, -0.02404345,\n",
      "       -0.01927286, -0.0150296 , -0.01038461, -0.0054352 , -0.00127384,\n",
      "        0.00345807,  0.00871412,  0.01680445,  0.02526902,  0.04430084,\n",
      "        0.08084569]), array([0.        , 0.09211807, 0.15249245, 0.17041227, 0.19909291,\n",
      "       0.21537773, 0.22882773, 0.24020888, 0.25525022, 0.26934806,\n",
      "       0.28031583, 0.29113985, 0.30599613, 0.33071827, 0.38791623,\n",
      "       0.55789922]), array([0.        , 0.09165776, 0.11262333, 0.12416132, 0.13371999,\n",
      "       0.14289789, 0.15036747, 0.15586185, 0.16460936, 0.17818376,\n",
      "       0.19770297, 0.21822798, 0.23498005, 0.25964075, 0.32326089,\n",
      "       0.5106016 ]), array([-0.07212874, -0.02239301,  0.        ,  0.00658281,  0.02201255,\n",
      "        0.03844857,  0.05031826,  0.06315689,  0.07301048,  0.08708748,\n",
      "        0.09957864,  0.11689396,  0.13441399,  0.16270747,  0.17974151]), array([0.        , 0.14631166, 0.23679746, 0.29871646, 0.34266691,\n",
      "       0.38205441, 0.42558447, 0.45728788, 0.49345977, 0.52589659,\n",
      "       0.55668182, 0.58954938, 0.62489634, 0.66610087]), array([0.        , 0.17177494, 0.23516907, 0.29175627, 0.33527199,\n",
      "       0.37387001, 0.40995326, 0.43627636, 0.46677745, 0.49598729,\n",
      "       0.52805416, 0.55465106, 0.58817157, 0.64227156]), array([-0.14832808, -0.09653794, -0.06427101, -0.03858462, -0.01603937,\n",
      "        0.        ,  0.01963088,  0.04304067,  0.06001109,  0.07951411,\n",
      "        0.09973335,  0.12732455,  0.15838369]), array([-0.10011709, -0.07514386, -0.05793237, -0.04614065, -0.0361379 ,\n",
      "       -0.02828493, -0.02077738, -0.01303229, -0.00558582,  0.        ,\n",
      "        0.00877968,  0.02275985,  0.03712272,  0.06195482,  0.09136871]), array([-0.01915073, -0.01201707, -0.00832598, -0.00559286, -0.00293503,\n",
      "        0.        ,  0.00175213,  0.00567647,  0.01080724]), array([0.03937212, 0.08305421, 0.1311798 , 0.1793881 , 0.20769402,\n",
      "       0.23311805, 0.25778286, 0.27982733, 0.30768221, 0.3346841 ,\n",
      "       0.36958606, 0.4080136 , 0.44913545, 0.51072784, 0.58232905,\n",
      "       0.67016625]), array([0.05968549, 0.12174764, 0.16142149, 0.19539719, 0.22427528,\n",
      "       0.24788965, 0.26919016, 0.29092662, 0.31242313, 0.3411751 ,\n",
      "       0.36900513, 0.40661851, 0.45494468, 0.50572056, 0.56430534,\n",
      "       0.6600338 ]), array([-0.6347571 , -0.54844156, -0.45679185, -0.39614709, -0.33644797,\n",
      "       -0.29586947, -0.26039426, -0.23238117, -0.20521335, -0.18174537,\n",
      "       -0.15577879, -0.12412183, -0.08848965, -0.04506571, -0.00151942,\n",
      "        0.05281246]), array([-0.60233978, -0.50151849, -0.43816001, -0.38894611, -0.33048228,\n",
      "       -0.29233519, -0.2610914 , -0.23404844, -0.20443214, -0.18077518,\n",
      "       -0.1594173 , -0.13530783, -0.10636559, -0.07406731, -0.01705537,\n",
      "        0.0398221 ]), array([-0.1517193 , -0.1126286 , -0.08601467, -0.0648607 , -0.046992  ,\n",
      "       -0.03291183, -0.01868728, -0.00752155,  0.00528188,  0.0172158 ,\n",
      "        0.02933744,  0.04249533,  0.05746504,  0.07616532,  0.10115907,\n",
      "        0.14485675]), array([ 3.8,  3.9,  4.9, 14.9, 16.2, 17.7]), array([0., 1.]), array([0., 1.]), array([0., 1.]), array([0.]), array([0., 1.]), array([0., 1.]), array([0., 1.]), array([0., 1.]), array([0., 1.]), array([0., 1.]), array([0., 1.]), array([0., 1.])], 'random_state': 42, 'w_over': (0.8, 1.2, 1.8), 'w_under': (1.0, 1.0, 1.0), 'over_power': 2.0, 'under_power': 2.0}\n",
      "Teacher fit seconds: 55.33\n",
      "\n",
      "=== TEACHER SUMMARY ===\n",
      "Teacher asym_loss (VAL) : 0.019742\n",
      "Teacher asym_loss (TEST): 0.020632\n",
      "\n",
      "Teacher mean strategy on TEST (pred vs true):\n",
      "  pred: check=0.3834, b13=0.3147, b23=0.3019\n",
      "  true: check=0.3885, b13=0.3097, b23=0.3019\n",
      "\n",
      "=== TEACHER mistake report (TEST) ===\n",
      "         model  asym_loss  mean_over_chk  mean_over_b13  mean_over_b23  big_misfire_b23_rate  big_misfire_b13_rate  MAE_chk  MAE_b13  MAE_b23\n",
      "TEACHER_FOREST   0.020632       0.053315       0.059306       0.049764                   0.0                   0.0 0.111702 0.113548 0.099521\n",
      "\n",
      "======================================================================\n",
      "=== DEPTH SWEEP (VALIDATION ONLY for choosing depth) ===\n",
      "Depths: [2, 3, 4, 5, 6, 7, 8, 9, 10] | min_leaf: [60, 90]\n",
      "\n",
      "=== TABLE 1: Fidelity vs Depth on VALIDATION ===\n",
      " depth  min_leaf  n_leaves  fit_seconds  fidelity_MSE_val\n",
      "     8        60        33        0.255          0.001500\n",
      "     9        60        33        0.255          0.001500\n",
      "    10        60        33        0.254          0.001500\n",
      "     7        60        31        0.255          0.001522\n",
      "     6        60        29        0.242          0.001589\n",
      "     5        60        22        0.223          0.002196\n",
      "     6        90        22        0.182          0.002424\n",
      "     7        90        22        0.182          0.002424\n",
      "     8        90        22        0.182          0.002424\n",
      "     9        90        22        0.182          0.002424\n",
      "    10        90        22        0.182          0.002424\n",
      "     5        90        19        0.178          0.002481\n",
      "     4        60        14        0.186          0.003059\n",
      "     4        90        14        0.172          0.003224\n",
      "     3        60         8        0.139          0.004774\n",
      "     3        90         8        0.128          0.004990\n",
      "     2        60         4        0.086          0.009208\n",
      "     2        90         4        0.083          0.009208\n",
      "\n",
      "=== TABLE 2: Validation Performance vs Depth ===\n",
      " depth  min_leaf  n_leaves  val_asym_loss_vs_TRUE  mean_pred_CHECK_val  mean_pred_BET13_val  mean_pred_BET23_val  mean_true_CHECK_val  mean_true_BET13_val  mean_true_BET23_val  mean_L1_gap_pred_vs_true_val  fidelity_hint\n",
      "     8        60        33               0.023768             0.394585             0.307237             0.298178             0.390265             0.309257             0.300478                      0.008640       0.001500\n",
      "     9        60        33               0.023768             0.394585             0.307237             0.298178             0.390265             0.309257             0.300478                      0.008640       0.001500\n",
      "    10        60        33               0.023768             0.394585             0.307237             0.298178             0.390265             0.309257             0.300478                      0.008640       0.001500\n",
      "     7        60        31               0.023966             0.394457             0.306917             0.298626             0.390265             0.309257             0.300478                      0.008384       0.001522\n",
      "     6        60        29               0.024174             0.394314             0.307349             0.298337             0.390265             0.309257             0.300478                      0.008099       0.001589\n",
      "     5        60        22               0.025830             0.394007             0.308000             0.297993             0.390265             0.309257             0.300478                      0.007484       0.002196\n",
      "     6        90        22               0.026794             0.393355             0.307913             0.298732             0.390265             0.309257             0.300478                      0.006181       0.002424\n",
      "     7        90        22               0.026794             0.393355             0.307913             0.298732             0.390265             0.309257             0.300478                      0.006181       0.002424\n",
      "     8        90        22               0.026794             0.393355             0.307913             0.298732             0.390265             0.309257             0.300478                      0.006181       0.002424\n",
      "     9        90        22               0.026794             0.393355             0.307913             0.298732             0.390265             0.309257             0.300478                      0.006181       0.002424\n",
      "    10        90        22               0.026794             0.393355             0.307913             0.298732             0.390265             0.309257             0.300478                      0.006181       0.002424\n",
      "     5        90        19               0.027011             0.392764             0.307872             0.299365             0.390265             0.309257             0.300478                      0.004998       0.002481\n",
      "     4        60        14               0.027582             0.394033             0.308668             0.297298             0.390265             0.309257             0.300478                      0.007537       0.003059\n",
      "     4        90        14               0.028393             0.392934             0.307805             0.299261             0.390265             0.309257             0.300478                      0.005339       0.003224\n",
      "     3        60         8               0.030728             0.391251             0.310509             0.298240             0.390265             0.309257             0.300478                      0.004475       0.004774\n",
      "     3        90         8               0.031491             0.391169             0.310457             0.298375             0.390265             0.309257             0.300478                      0.004207       0.004990\n",
      "     2        60         4               0.036654             0.392119             0.310357             0.297524             0.390265             0.309257             0.300478                      0.005908       0.009208\n",
      "     2        90         4               0.036654             0.392119             0.310357             0.297524             0.390265             0.309257             0.300478                      0.005908       0.009208\n",
      "\n",
      "=== DEPTH PICK HELPER (VALIDATION fidelity only) ===\n",
      "Best fidelity_MSE_val = 0.001500 | +5% cutoff = 0.001575\n",
      "Smallest tree within +5% fidelity: depth=7, min_leaf=60, n_leaves=31, fidelity_MSE_val=0.001522\n",
      "\n",
      "======================================================================\n",
      "=== FINAL BORN-AGAIN TREE (depth=4, min_leaf=60) ===\n",
      "Born-again fit seconds: 0.255 | n_leaves=15\n",
      "Born-again asym_loss vs TRUE (TEST): 0.026743\n",
      "Born-again fidelity MSE vs TEACHER (TEST): 0.002360\n",
      "\n",
      "Born-again mean strategy on TEST (pred vs true):\n",
      "  pred: check=0.3857, b13=0.3116, b23=0.3026\n",
      "  true: check=0.3885, b13=0.3097, b23=0.3019\n",
      "\n",
      "=== Born-Again mistake report (TEST) ===\n",
      "                  model  asym_loss  mean_over_chk  mean_over_b13  mean_over_b23  big_misfire_b23_rate  big_misfire_b13_rate  MAE_chk  MAE_b13  MAE_b23\n",
      "BORN_AGAIN_TREE_d4_ml60   0.026743       0.061667       0.067292       0.057081                   0.0                   0.0 0.126063 0.132636 0.113381\n",
      "\n",
      "=== BORN-AGAIN TREE RULES (for slides) ===\n",
      "Baseline (avg solver on TRAIN+VAL) = [check=0.38, b13=0.31, b23=0.31]\n",
      "IF SPR <= 4.9000:\n",
      "  IF STRONG_ADV <= -0.0150:\n",
      "    IF Flushes_MONOTONE <= 0.0000:\n",
      "      IF Straights_STRAIGHT <= 0.0000:\n",
      "        LEAF n=743 pred=[0.26, 0.24, 0.50] :: Lean BET23 (conf=0.50) | Δvs baseline: chk -0.12, b13 -0.07, b23 +0.19\n",
      "      ELSE:  # Straights_STRAIGHT > 0.0000\n",
      "        LEAF n=249 pred=[0.44, 0.19, 0.38] :: Mix (conf=0.44) | Δvs baseline: chk +0.06, b13 -0.13, b23 +0.07\n",
      "    ELSE:  # Flushes_MONOTONE > 0.0000\n",
      "      LEAF n=81 pred=[0.49, 0.34, 0.18] :: Lean CHECK (conf=0.49) | Δvs baseline: chk +0.10, b13 +0.02, b23 -0.13\n",
      "  ELSE:  # STRONG_ADV > -0.0150\n",
      "    IF VIL_TP_PLUS <= 0.1782:\n",
      "      IF VIL_STRONG <= 0.0969:\n",
      "        LEAF n=332 pred=[0.19, 0.45, 0.36] :: Lean BET13 (conf=0.45) | Δvs baseline: chk -0.19, b13 +0.14, b23 +0.05\n",
      "      ELSE:  # VIL_STRONG > 0.0969\n",
      "        LEAF n=203 pred=[0.28, 0.42, 0.30] :: Mix (conf=0.42) | Δvs baseline: chk -0.10, b13 +0.11, b23 -0.01\n",
      "    ELSE:  # VIL_TP_PLUS > 0.1782\n",
      "      IF HERO_AIR <= 0.1463:\n",
      "        LEAF n=77 pred=[0.27, 0.41, 0.32] :: Mix (conf=0.41) | Δvs baseline: chk -0.11, b13 +0.10, b23 +0.01\n",
      "      ELSE:  # HERO_AIR > 0.1463\n",
      "        LEAF n=336 pred=[0.42, 0.30, 0.28] :: Mix (conf=0.42) | Δvs baseline: chk +0.04, b13 -0.01, b23 -0.03\n",
      "ELSE:  # SPR > 4.9000\n",
      "  IF Position_IP <= 0.0000:\n",
      "    IF STRONG_ADV <= -0.0013:\n",
      "      IF VIL_POLAR <= -0.1064:\n",
      "        LEAF n=202 pred=[0.76, 0.15, 0.09] :: Mostly CHECK (conf=0.76) | Δvs baseline: chk +0.38, b13 -0.16, b23 -0.22\n",
      "      ELSE:  # VIL_POLAR > -0.1064\n",
      "        LEAF n=93 pred=[0.87, 0.08, 0.05] :: Mostly CHECK (conf=0.87) | Δvs baseline: chk +0.49, b13 -0.23, b23 -0.26\n",
      "    ELSE:  # STRONG_ADV > -0.0013\n",
      "      IF SPR <= 14.9000:\n",
      "        LEAF n=178 pred=[0.58, 0.28, 0.14] :: Lean CHECK (conf=0.58) | Δvs baseline: chk +0.20, b13 -0.03, b23 -0.17\n",
      "      ELSE:  # SPR > 14.9000\n",
      "        LEAF n=119 pred=[0.51, 0.37, 0.12] :: Lean CHECK (conf=0.51) | Δvs baseline: chk +0.13, b13 +0.06, b23 -0.18\n",
      "  ELSE:  # Position_IP > 0.0000\n",
      "    IF HERO_AIR <= 0.1463:\n",
      "      IF POLAR_ADV <= 0.0293:\n",
      "        LEAF n=74 pred=[0.25, 0.49, 0.26] :: Lean BET13 (conf=0.49) | Δvs baseline: chk -0.13, b13 +0.18, b23 -0.05\n",
      "      ELSE:  # POLAR_ADV > 0.0293\n",
      "        LEAF n=80 pred=[0.18, 0.68, 0.14] :: Mostly BET13 (conf=0.68) | Δvs baseline: chk -0.20, b13 +0.37, b23 -0.17\n",
      "    ELSE:  # HERO_AIR > 0.1463\n",
      "      IF AIR_ADV <= 0.0000:\n",
      "        LEAF n=419 pred=[0.35, 0.38, 0.28] :: Mix (conf=0.38) | Δvs baseline: chk -0.04, b13 +0.07, b23 -0.03\n",
      "      ELSE:  # AIR_ADV > 0.0000\n",
      "        LEAF n=227 pred=[0.45, 0.27, 0.28] :: Mix (conf=0.45) | Δvs baseline: chk +0.07, b13 -0.04, b23 -0.03\n",
      "\n",
      "======================================================================\n",
      "=== SIMPLE TREE (depth=4, min_leaf=60) trained on TRUE y ===\n",
      "Simple tree fit seconds: 0.231 | n_leaves=16\n",
      "Simple tree asym_loss vs TRUE (TEST): 0.027920\n",
      "\n",
      "Simple tree mean strategy on TEST (pred vs true):\n",
      "  pred: check=0.3771, b13=0.3196, b23=0.3032\n",
      "  true: check=0.3885, b13=0.3097, b23=0.3019\n",
      "\n",
      "=== Simple tree mistake report (TEST) ===\n",
      "              model  asym_loss  mean_over_chk  mean_over_b13  mean_over_b23  big_misfire_b23_rate  big_misfire_b13_rate  MAE_chk  MAE_b13  MAE_b23\n",
      "SIMPLE_TREE_d4_ml60    0.02792       0.062532       0.071923       0.055716                   0.0                   0.0 0.136413 0.133885 0.110044\n",
      "\n",
      "=== SIMPLE TREE RULES ===\n",
      "IF SPR <= 4.9000:\n",
      "  IF HERO_AIR <= 0.5259:\n",
      "    IF NUTS_ADV <= -0.0029:\n",
      "      IF HERO_POLAR <= -0.4568:\n",
      "        LEAF n=212 pred=[0.21, 0.16, 0.63] :: Mostly BET23 (conf=0.63) | Δvs baseline: chk -0.17, b13 -0.15, b23 +0.32\n",
      "      ELSE:  # HERO_POLAR > -0.4568\n",
      "        LEAF n=60 pred=[0.18, 0.43, 0.39] :: Mix (conf=0.43) | Δvs baseline: chk -0.20, b13 +0.11, b23 +0.09\n",
      "    ELSE:  # NUTS_ADV > -0.0029\n",
      "      IF Flushes_MONOTONE <= 0.0000:\n",
      "        LEAF n=1080 pred=[0.26, 0.41, 0.33] :: Mix (conf=0.41) | Δvs baseline: chk -0.12, b13 +0.10, b23 +0.02\n",
      "      ELSE:  # Flushes_MONOTONE > 0.0000\n",
      "        LEAF n=149 pred=[0.55, 0.31, 0.14] :: Lean CHECK (conf=0.55) | Δvs baseline: chk +0.17, b13 -0.01, b23 -0.16\n",
      "  ELSE:  # HERO_AIR > 0.5259\n",
      "    IF Straights_STRAIGHT <= 0.0000:\n",
      "      IF HERO_POLAR <= -0.0885:\n",
      "        LEAF n=116 pred=[0.23, 0.19, 0.59] :: Lean BET23 (conf=0.59) | Δvs baseline: chk -0.15, b13 -0.13, b23 +0.28\n",
      "      ELSE:  # HERO_POLAR > -0.0885\n",
      "        LEAF n=191 pred=[0.36, 0.14, 0.50] :: Lean BET23 (conf=0.50) | Δvs baseline: chk -0.02, b13 -0.18, b23 +0.20\n",
      "    ELSE:  # Straights_STRAIGHT > 0.0000\n",
      "      IF VIL_STRONG <= 0.0814:\n",
      "        LEAF n=142 pred=[0.46, 0.13, 0.42] :: Lean CHECK (conf=0.46) | Δvs baseline: chk +0.08, b13 -0.19, b23 +0.11\n",
      "      ELSE:  # VIL_STRONG > 0.0814\n",
      "        LEAF n=71 pred=[0.53, 0.19, 0.27] :: Lean CHECK (conf=0.53) | Δvs baseline: chk +0.15, b13 -0.12, b23 -0.03\n",
      "ELSE:  # SPR > 4.9000\n",
      "  IF Position_IP <= 0.0000:\n",
      "    IF STRONG_ADV <= -0.0013:\n",
      "      IF VIL_AIR <= 0.2918:\n",
      "        LEAF n=64 pred=[0.66, 0.21, 0.13] :: Mostly CHECK (conf=0.66) | Δvs baseline: chk +0.28, b13 -0.10, b23 -0.18\n",
      "      ELSE:  # VIL_AIR > 0.2918\n",
      "        LEAF n=231 pred=[0.84, 0.10, 0.06] :: Mostly CHECK (conf=0.84) | Δvs baseline: chk +0.46, b13 -0.21, b23 -0.25\n",
      "    ELSE:  # STRONG_ADV > -0.0013\n",
      "      IF SPR <= 14.9000:\n",
      "        LEAF n=178 pred=[0.59, 0.26, 0.14] :: Lean CHECK (conf=0.59) | Δvs baseline: chk +0.21, b13 -0.05, b23 -0.16\n",
      "      ELSE:  # SPR > 14.9000\n",
      "        LEAF n=119 pred=[0.48, 0.41, 0.11] :: Lean CHECK (conf=0.48) | Δvs baseline: chk +0.10, b13 +0.10, b23 -0.19\n",
      "  ELSE:  # Position_IP > 0.0000\n",
      "    IF HERO_AIR <= 0.1463:\n",
      "      IF POLAR_ADV <= 0.0053:\n",
      "        LEAF n=66 pred=[0.24, 0.47, 0.29] :: Lean BET13 (conf=0.47) | Δvs baseline: chk -0.14, b13 +0.16, b23 -0.02\n",
      "      ELSE:  # POLAR_ADV > 0.0053\n",
      "        LEAF n=88 pred=[0.18, 0.71, 0.12] :: Mostly BET13 (conf=0.71) | Δvs baseline: chk -0.20, b13 +0.39, b23 -0.19\n",
      "    ELSE:  # HERO_AIR > 0.1463\n",
      "      IF AIR_ADV <= -0.0160:\n",
      "        LEAF n=388 pred=[0.34, 0.39, 0.28] :: Mix (conf=0.39) | Δvs baseline: chk -0.05, b13 +0.07, b23 -0.03\n",
      "      ELSE:  # AIR_ADV > -0.0160\n",
      "        LEAF n=258 pred=[0.46, 0.26, 0.28] :: Lean CHECK (conf=0.46) | Δvs baseline: chk +0.08, b13 -0.05, b23 -0.03\n",
      "\n",
      "======================================================================\n",
      "=== SAVE CSV WITH PREDICTIONS (FULL FILTERED DATASET) ===\n",
      "Saved: Poker_semantic_context_models_FINAL.csv\n",
      "\n",
      "=== QUICK NUMBERS ===\n",
      "Teacher asym_loss TEST: 0.020632\n",
      "Born-again d4/ml60 asym_loss TEST: 0.026743 | fidelity MSE TEST: 0.002360 | leaves: 15\n",
      "Simple tree d4/ml60 asym_loss TEST: 0.027920 | leaves: 16\n"
     ]
    }
   ],
   "source": [
    "# Input:\n",
    "#   PokerData_with_range_features.csv\n",
    "#\n",
    "# Output:\n",
    "#   Poker_semantic_context_models_FINAL.csv\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0) CONFIG\n",
    "# ============================================================\n",
    "DATA_PATH = \"PokerData_with_range_features.csv\"\n",
    "OUT_PATH  = \"Poker_semantic_context_models_FINAL.csv\"\n",
    "\n",
    "BASE_RANDOM_STATE = 42\n",
    "np.random.seed(BASE_RANDOM_STATE)\n",
    "\n",
    "# Filter rule\n",
    "VIL_CHECKS_COL = \"% Villain Checks\"\n",
    "VIL_CHECKS_KEEP_THRESHOLD = 85.0\n",
    "\n",
    "# Targets (solver strategy)\n",
    "Y_COL_CHECK = \"We Check Back\"\n",
    "Y_COL_BET13 = \"We C-Bet 1/3\"\n",
    "Y_COL_BET23 = \"We C-Bet 2/3\"\n",
    "Y_COLS = [Y_COL_CHECK, Y_COL_BET13, Y_COL_BET23]\n",
    "\n",
    "# Context columns\n",
    "SPR_COL = \"SPR\"\n",
    "POS_COL = \"Position\"\n",
    "FULL_HOUSES_COL = \"Full Houses\"\n",
    "FLUSHES_COL = \"Flushes\"\n",
    "STRAIGHTS_COL = \"Straights\"\n",
    "CAT_COLS = [POS_COL, FULL_HOUSES_COL, FLUSHES_COL, STRAIGHTS_COL]\n",
    "\n",
    "# Asymmetric loss settings\n",
    "LOSS = dict(\n",
    "    w_over=(0.8, 1.2, 1.8),     # penalize over-betting more than over-check\n",
    "    w_under=(1.0, 1.0, 1.0),\n",
    "    over_power=2.0,\n",
    "    under_power=2.0\n",
    ")\n",
    "\n",
    "# Speed knobs\n",
    "GLOBAL_THRESHOLDS_PER_FEATURE = 16\n",
    "TUNING_SUBSAMPLE_N = 1200\n",
    "MAX_CONFIGS = 8\n",
    "N_SEEDS_FOR_STABILITY = 3\n",
    "\n",
    "# Teacher forest\n",
    "FINAL_TEACHER_N_TREES = 240\n",
    "\n",
    "# Final presentation trees (fixed)\n",
    "FINAL_DEPTH = 4\n",
    "FINAL_MIN_LEAF = 60\n",
    "\n",
    "# Depth sweep grid (for tables)\n",
    "DEPTH_GRID = list(range(2, 11))          # 2..10\n",
    "MIN_LEAF_GRID = [60, 90]                # include 60 (final choice) and 90 as a simpler alt\n",
    "SEED = BASE_RANDOM_STATE\n",
    "\n",
    "# Misfire thresholds (keep consistent across reports)\n",
    "BIG_MISFIRE_B23_PRED = 0.50\n",
    "BIG_MISFIRE_B23_TRUE = 0.10\n",
    "BIG_MISFIRE_B13_PRED = 0.60\n",
    "BIG_MISFIRE_B13_TRUE = 0.15\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Utilities\n",
    "# ============================================================\n",
    "def parse_percent_cell(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    if isinstance(x, (int, float, np.integer, np.floating)):\n",
    "        return float(x)\n",
    "    s = str(x).strip()\n",
    "    if s == \"\":\n",
    "        return np.nan\n",
    "    if s.endswith(\"%\"):\n",
    "        try:\n",
    "            return float(s[:-1])\n",
    "        except:\n",
    "            return np.nan\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def normalize_actions(y: np.ndarray) -> np.ndarray:\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    y = np.clip(y, 0.0, None)\n",
    "    s = y.sum(axis=1, keepdims=True)\n",
    "    s[s == 0] = 1.0\n",
    "    return y / s\n",
    "\n",
    "\n",
    "def require_cols(df: pd.DataFrame, cols: List[str], where: str = \"\"):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        msg = f\"Missing required columns {missing}\"\n",
    "        if where:\n",
    "            msg += f\" (needed for {where})\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "\n",
    "def safe_col(df: pd.DataFrame, name: str) -> pd.Series:\n",
    "    if name in df.columns:\n",
    "        return pd.to_numeric(df[name], errors=\"coerce\").fillna(0.0)\n",
    "    return pd.Series(0.0, index=df.index)\n",
    "\n",
    "\n",
    "def mean_vec(arr3):\n",
    "    return np.mean(normalize_actions(arr3), axis=0)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Asymmetric loss\n",
    "# ============================================================\n",
    "def asym_loss_from_err(err: np.ndarray,\n",
    "                       w_over=(0.8, 1.2, 1.8),\n",
    "                       w_under=(1.0, 1.0, 1.0),\n",
    "                       over_power=2.0,\n",
    "                       under_power=2.0) -> float:\n",
    "    over = np.maximum(err, 0.0)\n",
    "    under = np.maximum(-err, 0.0)\n",
    "    w_over = np.array(w_over, dtype=float).reshape(1, 3)\n",
    "    w_under = np.array(w_under, dtype=float).reshape(1, 3)\n",
    "    return float(np.mean(w_over * (over ** over_power) + w_under * (under ** under_power)))\n",
    "\n",
    "\n",
    "def asymmetric_loss(y_true: np.ndarray,\n",
    "                    y_pred: np.ndarray,\n",
    "                    w_over=(0.8, 1.2, 1.8),\n",
    "                    w_under=(1.0, 1.0, 1.0),\n",
    "                    over_power=2.0,\n",
    "                    under_power=2.0) -> float:\n",
    "    y_true = normalize_actions(y_true)\n",
    "    y_pred = normalize_actions(y_pred)\n",
    "    return asym_loss_from_err(y_pred - y_true, w_over, w_under, over_power, under_power)\n",
    "\n",
    "\n",
    "def node_loss_constant_pred(y_true: np.ndarray,\n",
    "                            pred_vec: np.ndarray,\n",
    "                            w_over, w_under,\n",
    "                            over_power, under_power) -> float:\n",
    "    y_true = normalize_actions(y_true)\n",
    "    pred_vec = normalize_actions(pred_vec.reshape(1, 3)).ravel()\n",
    "    err = pred_vec.reshape(1, 3) - y_true\n",
    "    return asym_loss_from_err(err, w_over, w_under, over_power, under_power)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Semantic features (compressed for interpretability)\n",
    "# ============================================================\n",
    "def add_semantic_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    out[\"HERO_NUTS\"] = (\n",
    "        safe_col(out, \"HeroMade_straight_flush\") +\n",
    "        safe_col(out, \"HeroMade_four_of_a_kind\") +\n",
    "        safe_col(out, \"HeroMade_full_house\")\n",
    "    )\n",
    "    out[\"VIL_NUTS\"] = (\n",
    "        safe_col(out, \"VillainMade_straight_flush\") +\n",
    "        safe_col(out, \"VillainMade_four_of_a_kind\") +\n",
    "        safe_col(out, \"VillainMade_full_house\")\n",
    "    )\n",
    "    out[\"NUTS_ADV\"] = out[\"HERO_NUTS\"] - out[\"VIL_NUTS\"]\n",
    "\n",
    "    out[\"HERO_STRONG\"] = (\n",
    "        safe_col(out, \"HeroMade_straight_flush\") +\n",
    "        safe_col(out, \"HeroMade_four_of_a_kind\") +\n",
    "        safe_col(out, \"HeroMade_full_house\") +\n",
    "        safe_col(out, \"HeroMade_flush\") +\n",
    "        safe_col(out, \"HeroMade_straight\") +\n",
    "        safe_col(out, \"HeroMade_set\") +\n",
    "        safe_col(out, \"HeroMade_trips\") +\n",
    "        safe_col(out, \"HeroMade_two_pair\")\n",
    "    )\n",
    "    out[\"VIL_STRONG\"] = (\n",
    "        safe_col(out, \"VillainMade_straight_flush\") +\n",
    "        safe_col(out, \"VillainMade_four_of_a_kind\") +\n",
    "        safe_col(out, \"VillainMade_full_house\") +\n",
    "        safe_col(out, \"VillainMade_flush\") +\n",
    "        safe_col(out, \"VillainMade_straight\") +\n",
    "        safe_col(out, \"VillainMade_set\") +\n",
    "        safe_col(out, \"VillainMade_trips\") +\n",
    "        safe_col(out, \"VillainMade_two_pair\")\n",
    "    )\n",
    "    out[\"STRONG_ADV\"] = out[\"HERO_STRONG\"] - out[\"VIL_STRONG\"]\n",
    "\n",
    "    out[\"HERO_TP_PLUS\"] = safe_col(out, \"HeroMade_top_pair\") + safe_col(out, \"HeroMade_overpair\")\n",
    "    out[\"VIL_TP_PLUS\"]  = safe_col(out, \"VillainMade_top_pair\") + safe_col(out, \"VillainMade_overpair\")\n",
    "    out[\"TP_PLUS_ADV\"]  = out[\"HERO_TP_PLUS\"] - out[\"VIL_TP_PLUS\"]\n",
    "\n",
    "    out[\"HERO_AIR\"] = safe_col(out, \"HeroMade_no_made_hand\") + safe_col(out, \"HeroMade_ace_high\")\n",
    "    out[\"VIL_AIR\"]  = safe_col(out, \"VillainMade_no_made_hand\") + safe_col(out, \"VillainMade_ace_high\")\n",
    "    out[\"AIR_ADV\"]  = out[\"HERO_AIR\"] - out[\"VIL_AIR\"]\n",
    "\n",
    "    out[\"DRAW_ADV\"] = (\n",
    "        safe_col(out, \"HeroDraw_flush_draw\") +\n",
    "        safe_col(out, \"HeroDraw_oesd\") +\n",
    "        safe_col(out, \"HeroDraw_gutshot\") +\n",
    "        safe_col(out, \"HeroDraw_combo_draw\")\n",
    "    ) - (\n",
    "        safe_col(out, \"VillainDraw_flush_draw\") +\n",
    "        safe_col(out, \"VillainDraw_oesd\") +\n",
    "        safe_col(out, \"VillainDraw_gutshot\") +\n",
    "        safe_col(out, \"VillainDraw_combo_draw\")\n",
    "    )\n",
    "    out[\"COMBO_DRAW_ADV\"] = safe_col(out, \"HeroDraw_combo_draw\") - safe_col(out, \"VillainDraw_combo_draw\")\n",
    "\n",
    "    out[\"HERO_MEDIUM\"] = (\n",
    "        safe_col(out, \"HeroMade_top_pair\") +\n",
    "        safe_col(out, \"HeroMade_second_pair\") +\n",
    "        safe_col(out, \"HeroMade_third_pair\") +\n",
    "        safe_col(out, \"HeroMade_underpair\")\n",
    "    )\n",
    "    out[\"VIL_MEDIUM\"] = (\n",
    "        safe_col(out, \"VillainMade_top_pair\") +\n",
    "        safe_col(out, \"VillainMade_second_pair\") +\n",
    "        safe_col(out, \"VillainMade_third_pair\") +\n",
    "        safe_col(out, \"VillainMade_underpair\")\n",
    "    )\n",
    "\n",
    "    out[\"HERO_POLAR\"] = (\n",
    "        out[\"HERO_NUTS\"] +\n",
    "        safe_col(out, \"HeroDraw_combo_draw\") +\n",
    "        safe_col(out, \"HeroDraw_flush_draw\") +\n",
    "        safe_col(out, \"HeroDraw_oesd\")\n",
    "    ) - out[\"HERO_MEDIUM\"]\n",
    "\n",
    "    out[\"VIL_POLAR\"] = (\n",
    "        out[\"VIL_NUTS\"] +\n",
    "        safe_col(out, \"VillainDraw_combo_draw\") +\n",
    "        safe_col(out, \"VillainDraw_flush_draw\") +\n",
    "        safe_col(out, \"VillainDraw_oesd\")\n",
    "    ) - out[\"VIL_MEDIUM\"]\n",
    "\n",
    "    out[\"POLAR_ADV\"] = out[\"HERO_POLAR\"] - out[\"VIL_POLAR\"]\n",
    "    return out\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Precompute thresholds ONCE\n",
    "# ============================================================\n",
    "def build_global_thresholds(X: np.ndarray, n_thresholds: int) -> List[np.ndarray]:\n",
    "    n, d = X.shape\n",
    "    qs = np.linspace(0.05, 0.95, n_thresholds)\n",
    "    thr = []\n",
    "    for j in range(d):\n",
    "        col = X[:, j]\n",
    "        if np.all(col == col[0]):\n",
    "            thr.append(np.array([], dtype=float))\n",
    "            continue\n",
    "        ths = np.unique(np.quantile(col, qs))\n",
    "        if len(ths) > n_thresholds:\n",
    "            idx = np.linspace(0, len(ths) - 1, n_thresholds).astype(int)\n",
    "            ths = ths[idx]\n",
    "        thr.append(ths.astype(float))\n",
    "    return thr\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Fast Tree + Forest\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class TreeNode:\n",
    "    is_leaf: bool\n",
    "    pred: np.ndarray\n",
    "    n: int\n",
    "    feat_idx: Optional[int] = None\n",
    "    thresh: Optional[float] = None\n",
    "    left: Optional[\"TreeNode\"] = None\n",
    "    right: Optional[\"TreeNode\"] = None\n",
    "\n",
    "\n",
    "class AsymmetricLossTreeFast:\n",
    "    def __init__(self,\n",
    "                 max_depth=5,\n",
    "                 min_leaf=60,\n",
    "                 max_features=0.7,  # int, fraction, or \"sqrt\"\n",
    "                 thresholds_by_feature=None,\n",
    "                 w_over=(0.8, 1.2, 1.8),\n",
    "                 w_under=(1.0, 1.0, 1.0),\n",
    "                 over_power=2.0,\n",
    "                 under_power=2.0,\n",
    "                 min_gain=1e-6,\n",
    "                 random_state=42):\n",
    "        self.max_depth = int(max_depth)\n",
    "        self.min_leaf = int(min_leaf)\n",
    "        self.max_features = max_features\n",
    "        self.thresholds_by_feature = thresholds_by_feature\n",
    "        self.w_over = tuple(w_over)\n",
    "        self.w_under = tuple(w_under)\n",
    "        self.over_power = float(over_power)\n",
    "        self.under_power = float(under_power)\n",
    "        self.min_gain = float(min_gain)\n",
    "        self.random_state = int(random_state)\n",
    "        self.root = None\n",
    "\n",
    "    def _node_pred_mean(self, y: np.ndarray) -> np.ndarray:\n",
    "        p = np.mean(y, axis=0)\n",
    "        return normalize_actions(p.reshape(1, 3)).ravel()\n",
    "\n",
    "    def _choose_feature_subset(self, d: int, rng: np.random.RandomState) -> np.ndarray:\n",
    "        if isinstance(self.max_features, str) and self.max_features == \"sqrt\":\n",
    "            k = max(1, int(math.sqrt(d)))\n",
    "        elif isinstance(self.max_features, float):\n",
    "            k = max(1, int(round(self.max_features * d)))\n",
    "        else:\n",
    "            k = max(1, min(int(self.max_features), d))\n",
    "        return rng.choice(d, size=k, replace=False)\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        if self.thresholds_by_feature is None:\n",
    "            raise ValueError(\"thresholds_by_feature required\")\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        self.root = self._build(X, y, depth=0, rng=rng)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        out = np.zeros((X.shape[0], 3), dtype=float)\n",
    "        for i in range(X.shape[0]):\n",
    "            node = self.root\n",
    "            while not node.is_leaf:\n",
    "                node = node.left if X[i, node.feat_idx] <= node.thresh else node.right\n",
    "            out[i] = node.pred\n",
    "        return normalize_actions(out)\n",
    "\n",
    "    def _build(self, X: np.ndarray, y: np.ndarray, depth: int, rng: np.random.RandomState) -> TreeNode:\n",
    "        n, d = X.shape\n",
    "        pred = self._node_pred_mean(y)\n",
    "\n",
    "        if depth >= self.max_depth or n < 2 * self.min_leaf:\n",
    "            return TreeNode(True, pred, n)\n",
    "\n",
    "        parent_loss = node_loss_constant_pred(y, pred, self.w_over, self.w_under, self.over_power, self.under_power)\n",
    "\n",
    "        feat_candidates = self._choose_feature_subset(d, rng)\n",
    "        best_gain, best = 0.0, None\n",
    "\n",
    "        for j in feat_candidates:\n",
    "            xj = X[:, j]\n",
    "            if np.all(xj == xj[0]):\n",
    "                continue\n",
    "            ths = self.thresholds_by_feature[j]\n",
    "            if ths is None or len(ths) == 0:\n",
    "                continue\n",
    "\n",
    "            for t in ths:\n",
    "                left_mask = (xj <= t)\n",
    "                nl = int(left_mask.sum())\n",
    "                nr = n - nl\n",
    "                if nl < self.min_leaf or nr < self.min_leaf:\n",
    "                    continue\n",
    "\n",
    "                yl, yr = y[left_mask], y[~left_mask]\n",
    "                pl, pr = self._node_pred_mean(yl), self._node_pred_mean(yr)\n",
    "\n",
    "                loss_l = node_loss_constant_pred(yl, pl, self.w_over, self.w_under, self.over_power, self.under_power)\n",
    "                loss_r = node_loss_constant_pred(yr, pr, self.w_over, self.w_under, self.over_power, self.under_power)\n",
    "\n",
    "                child_loss = (nl / n) * loss_l + (nr / n) * loss_r\n",
    "                gain = parent_loss - child_loss\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best = (j, float(t), left_mask)\n",
    "\n",
    "        if best is None or best_gain < self.min_gain:\n",
    "            return TreeNode(True, pred, n)\n",
    "\n",
    "        j, t, left_mask = best\n",
    "        left = self._build(X[left_mask], y[left_mask], depth + 1, rng)\n",
    "        right = self._build(X[~left_mask], y[~left_mask], depth + 1, rng)\n",
    "        return TreeNode(False, pred, n, feat_idx=j, thresh=t, left=left, right=right)\n",
    "\n",
    "\n",
    "class AsymmetricLossRandomForestFast:\n",
    "    def __init__(self,\n",
    "                 n_estimators=200,\n",
    "                 max_depth=5,\n",
    "                 min_leaf=60,\n",
    "                 max_features=0.7,\n",
    "                 thresholds_by_feature=None,\n",
    "                 bootstrap=True,\n",
    "                 w_over=(0.8, 1.2, 1.8),\n",
    "                 w_under=(1.0, 1.0, 1.0),\n",
    "                 over_power=2.0,\n",
    "                 under_power=2.0,\n",
    "                 random_state=42):\n",
    "        self.n_estimators = int(n_estimators)\n",
    "        self.max_depth = int(max_depth)\n",
    "        self.min_leaf = int(min_leaf)\n",
    "        self.max_features = max_features\n",
    "        self.thresholds_by_feature = thresholds_by_feature\n",
    "        self.bootstrap = bool(bootstrap)\n",
    "        self.w_over = tuple(w_over)\n",
    "        self.w_under = tuple(w_under)\n",
    "        self.over_power = float(over_power)\n",
    "        self.under_power = float(under_power)\n",
    "        self.random_state = int(random_state)\n",
    "        self.trees = []\n",
    "        self.boot_indices = []\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        if self.thresholds_by_feature is None:\n",
    "            raise ValueError(\"thresholds_by_feature required\")\n",
    "\n",
    "        n = X.shape[0]\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        self.trees, self.boot_indices = [], []\n",
    "\n",
    "        for b in range(self.n_estimators):\n",
    "            idx = rng.randint(0, n, size=n) if self.bootstrap else np.arange(n)\n",
    "            self.boot_indices.append(idx)\n",
    "\n",
    "            tree = AsymmetricLossTreeFast(\n",
    "                max_depth=self.max_depth,\n",
    "                min_leaf=self.min_leaf,\n",
    "                max_features=self.max_features,\n",
    "                thresholds_by_feature=self.thresholds_by_feature,\n",
    "                w_over=self.w_over,\n",
    "                w_under=self.w_under,\n",
    "                over_power=self.over_power,\n",
    "                under_power=self.under_power,\n",
    "                random_state=self.random_state + 1000 + b\n",
    "            )\n",
    "            tree.fit(X[idx], y[idx])\n",
    "            self.trees.append(tree)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        preds = np.zeros((X.shape[0], 3), dtype=float)\n",
    "        for t in self.trees:\n",
    "            preds += t.predict(X)\n",
    "        preds /= max(1, len(self.trees))\n",
    "        return normalize_actions(preds)\n",
    "\n",
    "    def oob_predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        n = X.shape[0]\n",
    "        acc = np.zeros((n, 3), dtype=float)\n",
    "        cnt = np.zeros(n, dtype=int)\n",
    "\n",
    "        for tree, idx in zip(self.trees, self.boot_indices):\n",
    "            inbag = np.zeros(n, dtype=bool)\n",
    "            inbag[idx] = True\n",
    "            oob_mask = ~inbag\n",
    "            if not np.any(oob_mask):\n",
    "                continue\n",
    "            acc[oob_mask] += tree.predict(X[oob_mask])\n",
    "            cnt[oob_mask] += 1\n",
    "\n",
    "        base = self.predict(X)\n",
    "        out = base.copy()\n",
    "        ok = cnt > 0\n",
    "        out[ok] = acc[ok] / cnt[ok].reshape(-1, 1)\n",
    "        return normalize_actions(out)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) Interpretability helpers\n",
    "# ============================================================\n",
    "ACTION_NAMES = [\"CHECK\", \"BET13\", \"BET23\"]\n",
    "\n",
    "def action_label(p: np.ndarray) -> str:\n",
    "    p = np.asarray(p, dtype=float).ravel()\n",
    "    top = int(np.argmax(p))\n",
    "    conf = float(np.max(p))\n",
    "    if conf >= 0.60:\n",
    "        return f\"Mostly {ACTION_NAMES[top]}\"\n",
    "    if conf >= 0.45:\n",
    "        return f\"Lean {ACTION_NAMES[top]}\"\n",
    "    return \"Mix\"\n",
    "\n",
    "def leaf_summary(pred: np.ndarray, baseline: np.ndarray) -> str:\n",
    "    pred = pred.ravel()\n",
    "    baseline = baseline.ravel()\n",
    "    lab = action_label(pred)\n",
    "    conf = float(np.max(pred))\n",
    "    d = pred - baseline\n",
    "    return (f\"{lab} (conf={conf:.2f}) | \"\n",
    "            f\"Δvs baseline: chk {d[0]:+.2f}, b13 {d[1]:+.2f}, b23 {d[2]:+.2f}\")\n",
    "\n",
    "def print_tree_rules_pretty(node: TreeNode, feature_names: List[str], baseline: np.ndarray, indent=\"\"):\n",
    "    if node.is_leaf:\n",
    "        p = node.pred\n",
    "        msg = leaf_summary(p, baseline)\n",
    "        print(f\"{indent}LEAF n={node.n} pred=[{p[0]:.2f}, {p[1]:.2f}, {p[2]:.2f}] :: {msg}\")\n",
    "        return\n",
    "    fname = feature_names[node.feat_idx]\n",
    "    print(f\"{indent}IF {fname} <= {node.thresh:.4f}:\")\n",
    "    print_tree_rules_pretty(node.left, feature_names, baseline, indent + \"  \")\n",
    "    print(f\"{indent}ELSE:  # {fname} > {node.thresh:.4f}\")\n",
    "    print_tree_rules_pretty(node.right, feature_names, baseline, indent + \"  \")\n",
    "\n",
    "def count_leaves(node: TreeNode) -> int:\n",
    "    if node.is_leaf:\n",
    "        return 1\n",
    "    return count_leaves(node.left) + count_leaves(node.right)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) Mistake analysis (presentation-friendly metrics)\n",
    "# ============================================================\n",
    "def mistake_report(y_true: np.ndarray,\n",
    "                   y_pred: np.ndarray,\n",
    "                   name: str,\n",
    "                   big_misfire_b23_pred=0.50,\n",
    "                   big_misfire_b23_true=0.10,\n",
    "                   big_misfire_b13_pred=0.60,\n",
    "                   big_misfire_b13_true=0.15) -> pd.DataFrame:\n",
    "    y_true = normalize_actions(y_true)\n",
    "    y_pred = normalize_actions(y_pred)\n",
    "    err = y_pred - y_true\n",
    "\n",
    "    over_b23_amt = float(np.mean(np.maximum(err[:, 2], 0.0)))\n",
    "    over_b13_amt = float(np.mean(np.maximum(err[:, 1], 0.0)))\n",
    "    over_chk_amt = float(np.mean(np.maximum(err[:, 0], 0.0)))\n",
    "\n",
    "    misfire_b23 = float(np.mean((y_pred[:, 2] >= big_misfire_b23_pred) & (y_true[:, 2] <= big_misfire_b23_true)))\n",
    "    misfire_b13 = float(np.mean((y_pred[:, 1] >= big_misfire_b13_pred) & (y_true[:, 1] <= big_misfire_b13_true)))\n",
    "\n",
    "    mae_chk = float(np.mean(np.abs(err[:, 0])))\n",
    "    mae_b13 = float(np.mean(np.abs(err[:, 1])))\n",
    "    mae_b23 = float(np.mean(np.abs(err[:, 2])))\n",
    "\n",
    "    out = pd.DataFrame([{\n",
    "        \"model\": name,\n",
    "        \"asym_loss\": asymmetric_loss(y_true, y_pred, **LOSS),\n",
    "        \"mean_over_chk\": over_chk_amt,\n",
    "        \"mean_over_b13\": over_b13_amt,\n",
    "        \"mean_over_b23\": over_b23_amt,\n",
    "        \"big_misfire_b23_rate\": misfire_b23,\n",
    "        \"big_misfire_b13_rate\": misfire_b13,\n",
    "        \"MAE_chk\": mae_chk,\n",
    "        \"MAE_b13\": mae_b13,\n",
    "        \"MAE_b23\": mae_b23\n",
    "    }])\n",
    "    return out\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) Load + filter + feature build\n",
    "# ============================================================\n",
    "print(\"=== LOAD DATA ===\")\n",
    "df0 = pd.read_csv(DATA_PATH)\n",
    "print(\"Rows / Cols:\", df0.shape)\n",
    "\n",
    "require_cols(df0, [VIL_CHECKS_COL] + Y_COLS, where=\"filter/targets\")\n",
    "df0[VIL_CHECKS_COL] = df0[VIL_CHECKS_COL].apply(parse_percent_cell)\n",
    "\n",
    "print(f\"\\n=== FILTER: keep ({VIL_CHECKS_COL} == -1) OR >= {VIL_CHECKS_KEEP_THRESHOLD} ===\")\n",
    "before = df0.shape[0]\n",
    "mask_keep = (df0[VIL_CHECKS_COL] == -1) | (df0[VIL_CHECKS_COL] >= VIL_CHECKS_KEEP_THRESHOLD)\n",
    "df = df0.loc[mask_keep].copy()\n",
    "after = df.shape[0]\n",
    "print(f\"Before: {before} | After: {after} | Dropped: {before-after}\")\n",
    "\n",
    "for c in Y_COLS:\n",
    "    df[c] = df[c].apply(parse_percent_cell)\n",
    "\n",
    "y_raw = df[Y_COLS].values.astype(float)\n",
    "if np.nanmax(y_raw) > 1.5:\n",
    "    print(\"Targets look like percents. Dividing by 100.\")\n",
    "    y_raw = y_raw / 100.0\n",
    "y = normalize_actions(y_raw)\n",
    "\n",
    "print(\"\\nTargets sanity check (first 5):\")\n",
    "tmp = pd.DataFrame(y[:5], columns=[\"CHECK\",\"BET13\",\"BET23\"])\n",
    "tmp[\"sum\"] = tmp.sum(axis=1)\n",
    "print(tmp)\n",
    "\n",
    "print(\"\\n=== ADD SEMANTIC FEATURES ===\")\n",
    "df = add_semantic_features(df)\n",
    "\n",
    "semantic_cols = [\n",
    "    \"HERO_NUTS\",\"VIL_NUTS\",\"NUTS_ADV\",\n",
    "    \"HERO_STRONG\",\"VIL_STRONG\",\"STRONG_ADV\",\n",
    "    \"HERO_TP_PLUS\",\"VIL_TP_PLUS\",\"TP_PLUS_ADV\",\n",
    "    \"HERO_AIR\",\"VIL_AIR\",\"AIR_ADV\",\n",
    "    \"DRAW_ADV\",\"COMBO_DRAW_ADV\",\n",
    "    \"HERO_MEDIUM\",\"VIL_MEDIUM\",\n",
    "    \"HERO_POLAR\",\"VIL_POLAR\",\"POLAR_ADV\"\n",
    "]\n",
    "require_cols(df, semantic_cols, where=\"semantic features\")\n",
    "\n",
    "feature_df = df[semantic_cols].copy()\n",
    "\n",
    "# SPR\n",
    "if SPR_COL in df.columns:\n",
    "    feature_df[SPR_COL] = pd.to_numeric(df[SPR_COL], errors=\"coerce\")\n",
    "    med = float(feature_df[SPR_COL].median()) if np.isfinite(feature_df[SPR_COL].median()) else 0.0\n",
    "    feature_df[SPR_COL] = feature_df[SPR_COL].fillna(med)\n",
    "else:\n",
    "    feature_df[SPR_COL] = 0.0\n",
    "\n",
    "# One-hot categories\n",
    "present_cat_cols = [c for c in CAT_COLS if c in df.columns]\n",
    "if present_cat_cols:\n",
    "    cats = df[present_cat_cols].astype(\"string\").fillna(\"nan\")\n",
    "    feature_df = pd.concat([feature_df, pd.get_dummies(cats, prefix=present_cat_cols, drop_first=False)], axis=1)\n",
    "\n",
    "feature_df = feature_df.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "X = feature_df.values.astype(float)\n",
    "feature_names = list(feature_df.columns)\n",
    "\n",
    "print(\"\\n=== FINAL FEATURE MATRIX ===\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"First 25 features:\", feature_names[:25])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) Clean split: Train / Val / Test (test never used for choices)\n",
    "# ============================================================\n",
    "print(\"\\n=== SPLITS (train/val/test) ===\")\n",
    "X_trainval, X_test, y_trainval, y_test, df_trainval, df_test = train_test_split(\n",
    "    X, y, df, test_size=0.20, random_state=BASE_RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val, df_train, df_val = train_test_split(\n",
    "    X_trainval, y_trainval, df_trainval, test_size=0.25, random_state=BASE_RANDOM_STATE\n",
    ")\n",
    "# => 60% train, 20% val, 20% test\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "\n",
    "print(\"\\n=== PRECOMPUTE GLOBAL THRESHOLDS (train only) ===\")\n",
    "t0 = time.time()\n",
    "thresholds_by_feature = build_global_thresholds(X_train, GLOBAL_THRESHOLDS_PER_FEATURE)\n",
    "print(f\"Done. seconds={time.time()-t0:.2f} | per-feature thresholds={GLOBAL_THRESHOLDS_PER_FEATURE}\")\n",
    "\n",
    "# tuning subset from TRAIN only\n",
    "rng = np.random.RandomState(BASE_RANDOM_STATE)\n",
    "if X_train.shape[0] > TUNING_SUBSAMPLE_N:\n",
    "    idx = rng.choice(X_train.shape[0], size=TUNING_SUBSAMPLE_N, replace=False)\n",
    "else:\n",
    "    idx = np.arange(X_train.shape[0])\n",
    "X_tune, y_tune = X_train[idx], y_train[idx]\n",
    "print(f\"\\n=== TUNING SUBSET ===\\nUsing {X_tune.shape[0]} / {X_train.shape[0]} training rows for tuning.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10) Tuning Teacher: choose best by VAL loss (stability across seeds)\n",
    "# ============================================================\n",
    "print(\"\\n=== TUNING TEACHER (pick by VAL loss; report seed variance) ===\")\n",
    "\n",
    "CANDIDATES = [\n",
    "    dict(n_estimators=140, max_depth=5, min_leaf=60, max_features=0.7, bootstrap=True, name=\"A\"),\n",
    "    dict(n_estimators=180, max_depth=5, min_leaf=60, max_features=0.7, bootstrap=True, name=\"B\"),\n",
    "    dict(n_estimators=200, max_depth=5, min_leaf=60, max_features=0.7, bootstrap=True, name=\"C\"),\n",
    "    dict(n_estimators=180, max_depth=4, min_leaf=60, max_features=0.7, bootstrap=True, name=\"D\"),\n",
    "    dict(n_estimators=180, max_depth=5, min_leaf=80, max_features=0.7, bootstrap=True, name=\"E\"),\n",
    "    dict(n_estimators=220, max_depth=5, min_leaf=60, max_features=0.8, bootstrap=True, name=\"F\"),\n",
    "    dict(n_estimators=160, max_depth=5, min_leaf=60, max_features=0.7, bootstrap=True, name=\"G\"),\n",
    "    dict(n_estimators=160, max_depth=4, min_leaf=60, max_features=0.7, bootstrap=True, name=\"H\"),\n",
    "]\n",
    "GRID = CANDIDATES[:MAX_CONFIGS]\n",
    "\n",
    "def fit_eval_once(cfg: Dict, seed: int) -> Dict:\n",
    "    forest = AsymmetricLossRandomForestFast(\n",
    "        n_estimators=cfg[\"n_estimators\"],\n",
    "        max_depth=cfg[\"max_depth\"],\n",
    "        min_leaf=cfg[\"min_leaf\"],\n",
    "        max_features=cfg[\"max_features\"],\n",
    "        thresholds_by_feature=thresholds_by_feature,\n",
    "        bootstrap=cfg[\"bootstrap\"],\n",
    "        random_state=seed,\n",
    "        **LOSS\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    forest.fit(X_tune, y_tune)\n",
    "    fit_sec = time.time() - t0\n",
    "\n",
    "    pred_tune = forest.predict(X_tune)\n",
    "    pred_val  = forest.predict(X_val)\n",
    "    pred_oob  = forest.oob_predict(X_tune)\n",
    "\n",
    "    return dict(\n",
    "        cfg_name=cfg[\"name\"],\n",
    "        seed=seed,\n",
    "        fit_seconds=fit_sec,\n",
    "        tune_loss=asymmetric_loss(y_tune, pred_tune, **LOSS),\n",
    "        val_loss=asymmetric_loss(y_val, pred_val, **LOSS),\n",
    "        oob_loss=asymmetric_loss(y_tune, pred_oob, **LOSS),\n",
    "    )\n",
    "\n",
    "tune_rows = []\n",
    "seeds = [BASE_RANDOM_STATE + 10*s for s in range(N_SEEDS_FOR_STABILITY)]\n",
    "\n",
    "for i, cfg in enumerate(GRID, start=1):\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(f\"CONFIG {i}/{len(GRID)} {cfg['name']}: {cfg}\")\n",
    "    per_seed = []\n",
    "    for seed in seeds:\n",
    "        r = fit_eval_once(cfg, seed)\n",
    "        per_seed.append(r)\n",
    "        print(f\"  seed={seed} | fit={r['fit_seconds']:.2f}s | tune={r['tune_loss']:.6f} | val={r['val_loss']:.6f} | oob={r['oob_loss']:.6f}\")\n",
    "    val_losses = [r[\"val_loss\"] for r in per_seed]\n",
    "    oob_losses = [r[\"oob_loss\"] for r in per_seed]\n",
    "    fit_secs   = [r[\"fit_seconds\"] for r in per_seed]\n",
    "    tune_rows.append(dict(\n",
    "        name=cfg[\"name\"],\n",
    "        n_estimators=cfg[\"n_estimators\"],\n",
    "        max_depth=cfg[\"max_depth\"],\n",
    "        min_leaf=cfg[\"min_leaf\"],\n",
    "        max_features=cfg[\"max_features\"],\n",
    "        bootstrap=cfg[\"bootstrap\"],\n",
    "        val_loss_mean=float(np.mean(val_losses)),\n",
    "        val_loss_std=float(np.std(val_losses)),\n",
    "        oob_loss_mean=float(np.mean(oob_losses)),\n",
    "        fit_seconds_mean=float(np.mean(fit_secs)),\n",
    "    ))\n",
    "\n",
    "tune_df = pd.DataFrame(tune_rows).sort_values([\"val_loss_mean\",\"val_loss_std\"]).reset_index(drop=True)\n",
    "print(\"\\n=== TEACHER TUNING SUMMARY (sorted by mean val loss, then stability) ===\")\n",
    "print(tune_df.to_string(index=False))\n",
    "\n",
    "best_row = tune_df.iloc[0].to_dict()\n",
    "print(\"\\n=== BEST TEACHER CONFIG (by VAL mean + stability) ===\")\n",
    "print(best_row)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 11) Fit FINAL Teacher on TRAIN+VAL, evaluate on VAL + TEST\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"=== FINAL TEACHER (fit TRAIN+VAL) ===\")\n",
    "\n",
    "X_tv = np.vstack([X_train, X_val])\n",
    "y_tv = np.vstack([y_train, y_val])\n",
    "\n",
    "teacher_params = dict(\n",
    "    n_estimators=FINAL_TEACHER_N_TREES,\n",
    "    max_depth=int(best_row[\"max_depth\"]),\n",
    "    min_leaf=int(best_row[\"min_leaf\"]),\n",
    "    max_features=float(best_row[\"max_features\"]),\n",
    "    bootstrap=bool(best_row[\"bootstrap\"]),\n",
    "    thresholds_by_feature=thresholds_by_feature,\n",
    "    random_state=BASE_RANDOM_STATE,\n",
    "    **LOSS\n",
    ")\n",
    "\n",
    "print(\"Teacher params:\", teacher_params)\n",
    "\n",
    "t0 = time.time()\n",
    "teacher = AsymmetricLossRandomForestFast(**teacher_params)\n",
    "teacher.fit(X_tv, y_tv)\n",
    "print(f\"Teacher fit seconds: {time.time()-t0:.2f}\")\n",
    "\n",
    "teacher_pred_val  = teacher.predict(X_val)\n",
    "teacher_pred_test = teacher.predict(X_test)\n",
    "\n",
    "teacher_val_loss  = asymmetric_loss(y_val,  teacher_pred_val,  **LOSS)\n",
    "teacher_test_loss = asymmetric_loss(y_test, teacher_pred_test, **LOSS)\n",
    "\n",
    "print(\"\\n=== TEACHER SUMMARY ===\")\n",
    "print(f\"Teacher asym_loss (VAL) : {teacher_val_loss:.6f}\")\n",
    "print(f\"Teacher asym_loss (TEST): {teacher_test_loss:.6f}\")\n",
    "\n",
    "teach_mean_pred_test = mean_vec(teacher_pred_test)\n",
    "true_mean_test = mean_vec(y_test)\n",
    "print(\"\\nTeacher mean strategy on TEST (pred vs true):\")\n",
    "print(f\"  pred: check={teach_mean_pred_test[0]:.4f}, b13={teach_mean_pred_test[1]:.4f}, b23={teach_mean_pred_test[2]:.4f}\")\n",
    "print(f\"  true: check={true_mean_test[0]:.4f}, b13={true_mean_test[1]:.4f}, b23={true_mean_test[2]:.4f}\")\n",
    "\n",
    "teacher_mist_test = mistake_report(\n",
    "    y_test, teacher_pred_test, name=\"TEACHER_FOREST\",\n",
    "    big_misfire_b23_pred=BIG_MISFIRE_B23_PRED,\n",
    "    big_misfire_b23_true=BIG_MISFIRE_B23_TRUE,\n",
    "    big_misfire_b13_pred=BIG_MISFIRE_B13_PRED,\n",
    "    big_misfire_b13_true=BIG_MISFIRE_B13_TRUE\n",
    ")\n",
    "print(\"\\n=== TEACHER mistake report (TEST) ===\")\n",
    "print(teacher_mist_test.to_string(index=False))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 12) Depth sweep (NO TEST LEAKAGE):\n",
    "#     - Students fit on TRAIN only (to mimic teacher on TRAIN)\n",
    "#     - Fidelity measured on VAL vs teacher preds on VAL\n",
    "#     - Performance measured on VAL vs TRUE y_val\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"=== DEPTH SWEEP (VALIDATION ONLY for choosing depth) ===\")\n",
    "print(\"Depths:\", DEPTH_GRID, \"| min_leaf:\", MIN_LEAF_GRID)\n",
    "\n",
    "teacher_pred_train = teacher.predict(X_train)\n",
    "teacher_pred_val   = teacher.predict(X_val)\n",
    "\n",
    "rows_fid = []\n",
    "rows_val = []\n",
    "\n",
    "for min_leaf in MIN_LEAF_GRID:\n",
    "    for depth in DEPTH_GRID:\n",
    "        t0 = time.time()\n",
    "        stud = AsymmetricLossTreeFast(\n",
    "            max_depth=depth,\n",
    "            min_leaf=min_leaf,\n",
    "            max_features=1.0,\n",
    "            thresholds_by_feature=thresholds_by_feature,\n",
    "            random_state=SEED,\n",
    "            **LOSS\n",
    "        )\n",
    "        stud.fit(X_train, teacher_pred_train)\n",
    "        fit_sec = time.time() - t0\n",
    "\n",
    "        n_leaves = count_leaves(stud.root)\n",
    "\n",
    "        stud_pred_val = stud.predict(X_val)\n",
    "\n",
    "        # Fidelity (VAL): student vs teacher\n",
    "        fid_mse_val = float(np.mean((stud_pred_val - teacher_pred_val) ** 2))\n",
    "\n",
    "        # Performance (VAL): student vs TRUE solver\n",
    "        val_loss_true = asymmetric_loss(y_val, stud_pred_val, **LOSS)\n",
    "\n",
    "        # mean strategy alignment (VAL) as a sanity check\n",
    "        stud_mean_val = mean_vec(stud_pred_val)\n",
    "        true_mean_val = mean_vec(y_val)\n",
    "        mean_L1_gap = float(np.sum(np.abs(stud_mean_val - true_mean_val)))\n",
    "\n",
    "        rows_fid.append(dict(\n",
    "            depth=depth,\n",
    "            min_leaf=min_leaf,\n",
    "            n_leaves=n_leaves,\n",
    "            fit_seconds=round(fit_sec, 3),\n",
    "            fidelity_MSE_val=fid_mse_val\n",
    "        ))\n",
    "\n",
    "        rows_val.append(dict(\n",
    "            depth=depth,\n",
    "            min_leaf=min_leaf,\n",
    "            n_leaves=n_leaves,\n",
    "            val_asym_loss_vs_TRUE=val_loss_true,\n",
    "            mean_pred_CHECK_val=stud_mean_val[0],\n",
    "            mean_pred_BET13_val=stud_mean_val[1],\n",
    "            mean_pred_BET23_val=stud_mean_val[2],\n",
    "            mean_true_CHECK_val=true_mean_val[0],\n",
    "            mean_true_BET13_val=true_mean_val[1],\n",
    "            mean_true_BET23_val=true_mean_val[2],\n",
    "            mean_L1_gap_pred_vs_true_val=mean_L1_gap,\n",
    "            fidelity_hint=fid_mse_val\n",
    "        ))\n",
    "\n",
    "fid_df = pd.DataFrame(rows_fid).sort_values([\"fidelity_MSE_val\",\"depth\",\"min_leaf\"]).reset_index(drop=True)\n",
    "val_df = pd.DataFrame(rows_val).sort_values([\"val_asym_loss_vs_TRUE\",\"depth\",\"min_leaf\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== TABLE 1: Fidelity vs Depth on VALIDATION ===\")\n",
    "print(fid_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== TABLE 2: Validation Performance vs Depth ===\")\n",
    "print(val_df.to_string(index=False))\n",
    "\n",
    "best_fid = float(fid_df[\"fidelity_MSE_val\"].min())\n",
    "close_cut = best_fid * 1.05\n",
    "cands = fid_df[fid_df[\"fidelity_MSE_val\"] <= close_cut].copy()\n",
    "cands = cands.sort_values([\"n_leaves\",\"depth\",\"min_leaf\",\"fidelity_MSE_val\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== DEPTH PICK HELPER (VALIDATION fidelity only) ===\")\n",
    "print(f\"Best fidelity_MSE_val = {best_fid:.6f} | +5% cutoff = {close_cut:.6f}\")\n",
    "if len(cands) == 0:\n",
    "    print(\"No candidate met +5% rule (unexpected). Consider +10%.\")\n",
    "else:\n",
    "    rec = cands.iloc[0].to_dict()\n",
    "    print(f\"Smallest tree within +5% fidelity: depth={int(rec['depth'])}, min_leaf={int(rec['min_leaf'])}, n_leaves={int(rec['n_leaves'])}, fidelity_MSE_val={rec['fidelity_MSE_val']:.6f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 13) FINAL Born-Again tree:\n",
    "#     depth=4, min_leaf=60\n",
    "#     Train on TRAIN+VAL to mimic teacher preds on TRAIN+VAL\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"=== FINAL BORN-AGAIN TREE (depth=4, min_leaf=60) ===\")\n",
    "\n",
    "teacher_pred_tv = teacher.predict(X_tv)\n",
    "\n",
    "t0 = time.time()\n",
    "born_again = AsymmetricLossTreeFast(\n",
    "    max_depth=FINAL_DEPTH,\n",
    "    min_leaf=FINAL_MIN_LEAF,\n",
    "    max_features=1.0,\n",
    "    thresholds_by_feature=thresholds_by_feature,\n",
    "    random_state=BASE_RANDOM_STATE,\n",
    "    **LOSS\n",
    ")\n",
    "born_again.fit(X_tv, teacher_pred_tv)\n",
    "fit_sec = time.time() - t0\n",
    "\n",
    "born_again_pred_test = born_again.predict(X_test)\n",
    "born_again_test_loss = asymmetric_loss(y_test, born_again_pred_test, **LOSS)\n",
    "born_again_fid_test  = float(np.mean((born_again_pred_test - teacher_pred_test) ** 2))\n",
    "born_again_leaves    = count_leaves(born_again.root)\n",
    "\n",
    "print(f\"Born-again fit seconds: {fit_sec:.3f} | n_leaves={born_again_leaves}\")\n",
    "print(f\"Born-again asym_loss vs TRUE (TEST): {born_again_test_loss:.6f}\")\n",
    "print(f\"Born-again fidelity MSE vs TEACHER (TEST): {born_again_fid_test:.6f}\")\n",
    "\n",
    "ba_mean_pred_test = mean_vec(born_again_pred_test)\n",
    "print(\"\\nBorn-again mean strategy on TEST (pred vs true):\")\n",
    "print(f\"  pred: check={ba_mean_pred_test[0]:.4f}, b13={ba_mean_pred_test[1]:.4f}, b23={ba_mean_pred_test[2]:.4f}\")\n",
    "print(f\"  true: check={true_mean_test[0]:.4f}, b13={true_mean_test[1]:.4f}, b23={true_mean_test[2]:.4f}\")\n",
    "\n",
    "born_again_mist_test = mistake_report(\n",
    "    y_test, born_again_pred_test, name=\"BORN_AGAIN_TREE_d4_ml60\",\n",
    "    big_misfire_b23_pred=BIG_MISFIRE_B23_PRED,\n",
    "    big_misfire_b23_true=BIG_MISFIRE_B23_TRUE,\n",
    "    big_misfire_b13_pred=BIG_MISFIRE_B13_PRED,\n",
    "    big_misfire_b13_true=BIG_MISFIRE_B13_TRUE\n",
    ")\n",
    "print(\"\\n=== Born-Again mistake report (TEST) ===\")\n",
    "print(born_again_mist_test.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== BORN-AGAIN TREE RULES (for slides) ===\")\n",
    "baseline = mean_vec(y_tv)\n",
    "print(f\"Baseline (avg solver on TRAIN+VAL) = [check={baseline[0]:.2f}, b13={baseline[1]:.2f}, b23={baseline[2]:.2f}]\")\n",
    "print_tree_rules_pretty(born_again.root, feature_names, baseline)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 14) Simple Decision Tree baseline (same size as born-again):\n",
    "#     depth=4, min_leaf=60\n",
    "#     Train on TRAIN+VAL to predict TRUE solver y\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"=== SIMPLE TREE (depth=4, min_leaf=60) trained on TRUE y ===\")\n",
    "\n",
    "t0 = time.time()\n",
    "simple_tree = AsymmetricLossTreeFast(\n",
    "    max_depth=FINAL_DEPTH,\n",
    "    min_leaf=FINAL_MIN_LEAF,\n",
    "    max_features=1.0,\n",
    "    thresholds_by_feature=thresholds_by_feature,\n",
    "    random_state=BASE_RANDOM_STATE,\n",
    "    **LOSS\n",
    ")\n",
    "simple_tree.fit(X_tv, y_tv)\n",
    "fit_sec = time.time() - t0\n",
    "\n",
    "simple_pred_test = simple_tree.predict(X_test)\n",
    "simple_test_loss = asymmetric_loss(y_test, simple_pred_test, **LOSS)\n",
    "simple_leaves    = count_leaves(simple_tree.root)\n",
    "\n",
    "print(f\"Simple tree fit seconds: {fit_sec:.3f} | n_leaves={simple_leaves}\")\n",
    "print(f\"Simple tree asym_loss vs TRUE (TEST): {simple_test_loss:.6f}\")\n",
    "\n",
    "simp_mean_pred_test = mean_vec(simple_pred_test)\n",
    "print(\"\\nSimple tree mean strategy on TEST (pred vs true):\")\n",
    "print(f\"  pred: check={simp_mean_pred_test[0]:.4f}, b13={simp_mean_pred_test[1]:.4f}, b23={simp_mean_pred_test[2]:.4f}\")\n",
    "print(f\"  true: check={true_mean_test[0]:.4f}, b13={true_mean_test[1]:.4f}, b23={true_mean_test[2]:.4f}\")\n",
    "\n",
    "simple_mist_test = mistake_report(\n",
    "    y_test, simple_pred_test, name=\"SIMPLE_TREE_d4_ml60\",\n",
    "    big_misfire_b23_pred=BIG_MISFIRE_B23_PRED,\n",
    "    big_misfire_b23_true=BIG_MISFIRE_B23_TRUE,\n",
    "    big_misfire_b13_pred=BIG_MISFIRE_B13_PRED,\n",
    "    big_misfire_b13_true=BIG_MISFIRE_B13_TRUE\n",
    ")\n",
    "print(\"\\n=== Simple tree mistake report (TEST) ===\")\n",
    "print(simple_mist_test.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== SIMPLE TREE RULES ===\")\n",
    "print_tree_rules_pretty(simple_tree.root, feature_names, baseline)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 15) Save CSV with predictions (full filtered dataset)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"=== SAVE CSV WITH PREDICTIONS (FULL FILTERED DATASET) ===\")\n",
    "\n",
    "teacher_pred_all   = teacher.predict(X)\n",
    "born_again_pred_all = born_again.predict(X)\n",
    "simple_pred_all     = simple_tree.predict(X)\n",
    "\n",
    "df_out = df.copy()\n",
    "df_out[\"Y_CHECK\"] = y[:, 0]\n",
    "df_out[\"Y_BET13\"] = y[:, 1]\n",
    "df_out[\"Y_BET23\"] = y[:, 2]\n",
    "\n",
    "df_out[\"PRED_TEACHER_CHECK\"] = teacher_pred_all[:, 0]\n",
    "df_out[\"PRED_TEACHER_BET13\"] = teacher_pred_all[:, 1]\n",
    "df_out[\"PRED_TEACHER_BET23\"] = teacher_pred_all[:, 2]\n",
    "\n",
    "df_out[\"PRED_BORN_AGAIN_CHECK\"] = born_again_pred_all[:, 0]\n",
    "df_out[\"PRED_BORN_AGAIN_BET13\"] = born_again_pred_all[:, 1]\n",
    "df_out[\"PRED_BORN_AGAIN_BET23\"] = born_again_pred_all[:, 2]\n",
    "\n",
    "df_out[\"PRED_SIMPLE_CHECK\"] = simple_pred_all[:, 0]\n",
    "df_out[\"PRED_SIMPLE_BET13\"] = simple_pred_all[:, 1]\n",
    "df_out[\"PRED_SIMPLE_BET23\"] = simple_pred_all[:, 2]\n",
    "\n",
    "df_out.to_csv(OUT_PATH, index=False)\n",
    "print(\"Saved:\", OUT_PATH)\n",
    "\n",
    "print(\"\\n=== QUICK NUMBERS ===\")\n",
    "print(f\"Teacher asym_loss TEST: {teacher_test_loss:.6f}\")\n",
    "print(f\"Born-again d4/ml60 asym_loss TEST: {born_again_test_loss:.6f} | fidelity MSE TEST: {born_again_fid_test:.6f} | leaves: {born_again_leaves}\")\n",
    "print(f\"Simple tree d4/ml60 asym_loss TEST: {simple_test_loss:.6f} | leaves: {simple_leaves}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ac3610-d599-4621-b750-201f81fecd11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
